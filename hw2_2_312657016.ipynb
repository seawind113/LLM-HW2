{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryYjDmh_zJu1"
      },
      "source": [
        "## Presteps to Load llama3.2 On Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ExTu3CzJu2",
        "outputId": "80ba2d5b-6dee-4f4f-b358-ab2cc938ca2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Info: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Total RAM: 12.67477035522461 GB\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Check GPU\n",
        "gpu_info = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU Info: {gpu_info}\")\n",
        "\n",
        "# Check RAM\n",
        "ram_info = virtual_memory()\n",
        "print(f\"Total RAM: {ram_info.total / (1024**3)} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQPGpFFpzJu2",
        "outputId": "dbca23e8-e5f6-46cd-ddb5-eb905dff9bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNE89yuwzJu3",
        "outputId": "153cbcac-389d-4208-dcf1-f3e1d3e00a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...   0% ▕▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...   0% ▕▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...   4% ▕▏  11 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  10% ▕▏  28 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  23% ▕▏  62 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  35% ▕▏  94 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  40% ▕▏ 109 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  50% ▕▏ 137 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  55% ▕▏ 150 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  65% ▕▏ 179 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  70% ▕▏ 192 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  70% ▕▏ 193 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  70% ▕▏ 193 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  71% ▕▏ 193 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  71% ▕▏ 193 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  71% ▕▏ 194 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  71% ▕▏ 194 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  71% ▕▏ 194 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  71% ▕▏ 195 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  75% ▕▏ 206 MB/274 MB  192 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  80% ▕▏ 220 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  84% ▕▏ 231 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  88% ▕▏ 241 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  90% ▕▏ 248 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  92% ▕▏ 252 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  95% ▕▏ 261 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  97% ▕▏ 265 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90...  98% ▕▏ 268 MB/274 MB  110 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917...   0% ▕▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917...   0% ▕▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046...   0% ▕▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046...   0% ▕▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa...   0% ▕▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa...   0% ▕▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.2:3b  & ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a_NK54pzJu3"
      },
      "source": [
        "## Presteps to Load llama3.2 Locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSVRU-2GzJu3"
      },
      "source": [
        "**Hardware Requirements** <br>\n",
        "**CPU**: Multicore processor<br>\n",
        "**RAM**: Minimum of 16 GB recommended<br>\n",
        "**GPU**: NVIDIA RTX series (for optimal performance), at least 8 GB VRAM<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7a9-ipFzJu3"
      },
      "source": [
        "**Step1**:<br>\n",
        "Download ollama from this site according to your operating system<br>\n",
        "https://ollama.com/download/linux<br>\n",
        "<br>\n",
        "**Step2**:<br>\n",
        "open your teminal<br>\n",
        "<br>\n",
        "**Step3**:<br>\n",
        "run following commands in your terminal<br>\n",
        "\\$ ollama serve<br>\n",
        "\\$ ollama pull llama3.2:3b  & ollama pull nomic-embed-text<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qXOubp4zJu3"
      },
      "source": [
        "## Load LlaMA3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0bnqQW1zJu3",
        "outputId": "2fcb3a41-f138-4eac-e521-69630d5ccdb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from -r /content/RAG requirements.txt (line 1)) (0.3.12)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (from -r /content/RAG requirements.txt (line 2)) (0.3.12)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (from -r /content/RAG requirements.txt (line 3)) (0.5.23)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from -r /content/RAG requirements.txt (line 4)) (5.1.0)\n",
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.10/dist-packages (from -r /content/RAG requirements.txt (line 5)) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r /content/RAG requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community->-r /content/RAG requirements.txt (line 2)) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community->-r /content/RAG requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community->-r /content/RAG requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.115.6)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/RAG requirements.txt (line 3)) (0.34.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (3.7.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (1.29.0)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (4.66.6)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (1.68.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (31.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r /content/RAG requirements.txt (line 3)) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r /content/RAG requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->-r /content/RAG requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->-r /content/RAG requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->-r /content/RAG requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/RAG requirements.txt (line 2)) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/RAG requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb->-r /content/RAG requirements.txt (line 3)) (0.41.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain->-r /content/RAG requirements.txt (line 1)) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->-r /content/RAG requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/RAG requirements.txt (line 3)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/RAG requirements.txt (line 3)) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/RAG requirements.txt (line 3)) (5.29.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/RAG requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb->-r /content/RAG requirements.txt (line 3)) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.66.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.29.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.50b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/RAG requirements.txt (line 3)) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->-r /content/RAG requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r /content/RAG requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r /content/RAG requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community->-r /content/RAG requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r /content/RAG requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb->-r /content/RAG requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb->-r /content/RAG requirements.txt (line 3)) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r /content/RAG requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb->-r /content/RAG requirements.txt (line 3)) (0.26.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->-r /content/RAG requirements.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/RAG requirements.txt (line 3)) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/RAG requirements.txt (line 3)) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/RAG requirements.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/RAG requirements.txt (line 3)) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb->-r /content/RAG requirements.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb->-r /content/RAG requirements.txt (line 3)) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r /content/RAG requirements.txt (line 3)) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain->-r /content/RAG requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb->-r /content/RAG requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/RAG requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r /content/RAG requirements.txt (line 3)) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r /content/RAG requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/RAG requirements.txt (line 3)) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r \"/content/RAG requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfpy7Ez7UN4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bacde5ba-7f99-4847-ebe8-abe619d5bc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 970aa74c0a90... 100% ▕▏ 274 MB                         \n",
            "pulling c71d239df917... 100% ▕▏  11 KB                         \n",
            "pulling ce4a164fc046... 100% ▕▏   17 B                         \n",
            "pulling 31df23ea7daa... 100% ▕▏  420 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "MODEL = \"llama3.2:3b\"\n",
        "\n",
        "# Initialize the Llama model\n",
        "model = Ollama(model=MODEL)\n",
        "\n",
        "# Create an embedding model\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86LvINhAWLl-",
        "outputId": "0750a0bb-c2a7-4332-f67c-593d0047ee24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Yes, I'm an instance of Llama, a large language model developed by Meta. I'm designed to process and generate human-like text based on the input I receive. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "print(model.invoke(\"Hi. Are you LlaMA, the language model?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0lRQIuDFw1"
      },
      "source": [
        "## Part1 Standard RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_93yT72DHsW"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
        "from chromadb.errors import InvalidDimensionException\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "loader = PyPDFLoader(\"/content/RAG_survey.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "## NOTE: you must run Chroma().delete_collection() before load the Chroma vectorstore\n",
        "## to delete previous loaded documents.\n",
        "Chroma().delete_collection()\n",
        "vectorstore = Chroma.from_documents(documents = splits, embedding=embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRrfNphCzJu4"
      },
      "source": [
        "### (a) Chain the Components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YDD828UzJu4"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Chain\n",
        "\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=retriever      #用前面format_docs定義的方式來抓文件\n",
        "    ) |                 # | 就代表串在(chain)一起了\n",
        "    RunnableMap({\n",
        "        \"context\": lambda x: x['context'],\n",
        "        \"question\": lambda x: x['question']\n",
        "    }) |\n",
        "    (lambda x: f\"Context:\\n{x['context']}\\n\\nQuestion:\\n{x['question']}\") |\n",
        "    llm |\n",
        "    output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Llhq4I8AzJu4",
        "outputId": "f38f52b3-9930-434d-fcd8-0082c91a1c68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text does not explicitly state that implementing RAG is difficult. However, it mentions some challenges that RAG currently faces (Section VII), such as:\\n\\n* The need for continuous knowledge updates\\n* Integration of domain-specific information\\n* Wide adoption of ChatGPT\\n\\nIt also discusses the difficulties in addressing these challenges and points out prospective avenues for research and development.\\n\\nAdditionally, the text mentions that there are different versions of RAG paradigms (Naive RAG, Advanced RAG, Modular RAG), which might imply that implementing RAG can be complex due to the need to understand and integrate these different components.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "rag_chain.invoke({\"context\": retriever, \"question\" : \"what is this paper about?\"})\n",
        "rag_chain.invoke({\"context\": retriever, \"question\" : \"Is it difficult to implement RAG ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrfrvekYzJu4"
      },
      "source": [
        "### (b) Explain TextSplitter Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToRXYjfEzJu4"
      },
      "source": [
        "Discussion: 作業所使用的Text splitter為RecursiveCharacterTextSplitter，因為語言模型能接受的token數量有限，所以要對文件先進行前處理，Text splitter的功能就是先將文本按照字符(character)切成區塊(chunk)，再根據區塊長度(chunk size)將這些區塊結合起來，但如果每一個區塊都沒有重疊的話，就可能會遺失上下文的重要資訊，所以可以透過設定區塊重疊(chunk overlap)的大小來保留上下文的資訊。\n",
        "\n",
        "\n",
        "一開始設定的區塊長度為500，區塊重疊大小為100，就是會每個區塊長度為500個字符，然後每個區塊前後會有100個字符是重疊的，此時詢問RAG論文的內容回答如下:\n",
        "\"This paper appears to be about the Reader-Actor-Generator (RAG) system, specifically its indexing and retrieval phases. It explains how the RAG system processes documents, encodes them into vectors, and retrieves relevant chunks based on semantic similarity scores. The paper seems to focus on optimizing the indexing phase to improve the efficiency of the system's performance in answering questions.\"\n",
        "\n",
        "\n",
        "現在嘗試把區塊長度改為100，區塊重疊大小改為50，一樣的問題回答如下：\n",
        "\n",
        "Based on the provided snippet, it appears that this paper is discussing vector databases and their application in information retrieval (IR). The text mentions indexing, parallel search capabilities, and diversifying results for different query perspectives.\n",
        "\n",
        "Given the title of the PDF, \"RAG Survey\", I would venture to guess that this paper is a survey or overview of the state-of-the-art in vector-based systems, possibly covering aspects such as:\n",
        "\n",
        "1. Vector databases (e.g., Faiss, Annoy)\n",
        "2. Indexing and search techniques\n",
        "3. Applications in IR, NLP, and other areas\n",
        "\n",
        "However, without reading the full text, I can only make an educated guess about the specific focus of the paper.\n",
        "\n",
        "回答就多了不確定的成分在裡面，摘要也顯得冗長，所以適當的區塊長度與重疊大小對於分析文本是不可或缺的。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLdej0TkzJu4"
      },
      "source": [
        "### (c) Experiment with Retriever Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 把搜尋文件數量從預設的4改為5"
      ],
      "metadata": {
        "id": "GpXv8JN5hqpk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k820dQljzJu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c374326a-966a-4f64-ef2b-c65f6cf021de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "page_content='The paper unfolds as follows: Section II introduces the\n",
            "main concept and current paradigms of RAG. The following\n",
            "three sections explore core components—“Retrieval”, “Gen-\n",
            "eration” and “Augmentation”, respectively. Section III focuses\n",
            "on optimization methods in retrieval,including indexing, query\n",
            "and embedding optimization. Section IV concentrates on post-\n",
            "retrieval process and LLM fine-tuning in generation. Section V\n",
            "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='analyzes the three augmentation processes. Section VI focuses\n",
            "on RAG’s downstream tasks and evaluation system. Sec-\n",
            "tion VII mainly discusses the challenges that RAG currently\n",
            "faces and its future development directions. At last, the paper\n",
            "concludes in Section VIII.\n",
            "II. O VERVIEW OF RAG\n",
            "A typical application of RAG is illustrated in Figure 2.\n",
            "Here, a user poses a question to ChatGPT about a recent,\n",
            "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='situation is figuratively referred to as “Misinformation can\n",
            "be worse than no information at all”. Improving RAG’s\n",
            "resistance to such adversarial or counterfactual inputs is gain-\n",
            "ing research momentum and has become a key performance\n",
            "metric [48], [50], [82]. Cuconasu et al. [54] analyze which\n",
            "type of documents should be retrieved, evaluate the relevance\n",
            "of the documents to the prompt, their position, and the\n",
            "number included in the context. The research findings reveal' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='VIII. C ONCLUSION\n",
            "The summary of this paper, as depicted in Figure 6, empha-\n",
            "sizes RAG’s significant advancement in enhancing the capa-\n",
            "bilities of LLMs by integrating parameterized knowledge from\n",
            "language models with extensive non-parameterized data from\n",
            "external knowledge bases. The survey showcases the evolution\n",
            "of RAG technologies and their application on many different\n",
            "tasks. The analysis outlines three developmental paradigms\n",
            "within the RAG framework: Naive, Advanced, and Modu-' metadata={'page': 15, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='ponents intricately collaborate to form a cohesive and\n",
            "effective RAG framework.\n",
            "• We have summarized the current assessment methods of\n",
            "RAG, covering 26 tasks, nearly 50 datasets, outlining\n",
            "the evaluation objectives and metrics, as well as the\n",
            "current evaluation benchmarks and tools. Additionally,\n",
            "we anticipate future directions for RAG, emphasizing\n",
            "potential enhancements to tackle current challenges.\n",
            "The paper unfolds as follows: Section II introduces the' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}\n"
          ]
        }
      ],
      "source": [
        "## TODO: Try some different settings for the retriever and output some examples\n",
        "## you can cahnge the question if you want\n",
        "## you can duplicate this cell to ouput different examples\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "retrived_docs= retriever.invoke(\"what is this paper about?\")\n",
        "for doc in retrived_docs:\n",
        "    print()\n",
        "    print(doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 以Similarity score門檻來提取文件\n",
        "\n"
      ],
      "metadata": {
        "id": "WKnleqOdiZ6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKJyCKF1hn_N",
        "outputId": "da6b1248-01f5-48ac-cdef-605b5bd494d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(Document(metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}, page_content='The paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses'), 326.7880554199219)\n",
            "\n",
            "(Document(metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}, page_content='analyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. O VERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-'), 334.93194580078125)\n",
            "\n",
            "(Document(metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}, page_content='situation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal'), 341.1283264160156)\n",
            "\n",
            "(Document(metadata={'page': 15, 'source': '/content/RAG_survey.pdf'}, page_content='VIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-'), 343.9747009277344)\n"
          ]
        }
      ],
      "source": [
        "## TODO: Try some different settings for the retriever and output some examples\n",
        "## you can cahnge the question if you want\n",
        "## you can duplicate this cell to ouput different examples\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\n",
        "        \"k\": 5,  # 最多提取5份文件\n",
        "        \"score_threshold\": 0.7  # 相似度門檻設定為0.7\n",
        "    }\n",
        ")\n",
        "retrived_docs= vectorstore.similarity_search_with_score(\"what is this paper about?\")\n",
        "for doc in retrived_docs:\n",
        "    print()\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 以Maximal Marginal Relevance (MMR) 來提取"
      ],
      "metadata": {
        "id": "yI2mQ4tvjkWJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miC7YOUshpHX",
        "outputId": "433907aa-dbf3-4fa2-89cd-7087d09ef5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "page_content='The paper unfolds as follows: Section II introduces the\n",
            "main concept and current paradigms of RAG. The following\n",
            "three sections explore core components—“Retrieval”, “Gen-\n",
            "eration” and “Augmentation”, respectively. Section III focuses\n",
            "on optimization methods in retrieval,including indexing, query\n",
            "and embedding optimization. Section IV concentrates on post-\n",
            "retrieval process and LLM fine-tuning in generation. Section V\n",
            "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='analyzes the three augmentation processes. Section VI focuses\n",
            "on RAG’s downstream tasks and evaluation system. Sec-\n",
            "tion VII mainly discusses the challenges that RAG currently\n",
            "faces and its future development directions. At last, the paper\n",
            "concludes in Section VIII.\n",
            "II. O VERVIEW OF RAG\n",
            "A typical application of RAG is illustrated in Figure 2.\n",
            "Here, a user poses a question to ChatGPT about a recent,\n",
            "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='situation is figuratively referred to as “Misinformation can\n",
            "be worse than no information at all”. Improving RAG’s\n",
            "resistance to such adversarial or counterfactual inputs is gain-\n",
            "ing research momentum and has become a key performance\n",
            "metric [48], [50], [82]. Cuconasu et al. [54] analyze which\n",
            "type of documents should be retrieved, evaluate the relevance\n",
            "of the documents to the prompt, their position, and the\n",
            "number included in the context. The research findings reveal' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='VIII. C ONCLUSION\n",
            "The summary of this paper, as depicted in Figure 6, empha-\n",
            "sizes RAG’s significant advancement in enhancing the capa-\n",
            "bilities of LLMs by integrating parameterized knowledge from\n",
            "language models with extensive non-parameterized data from\n",
            "external knowledge bases. The survey showcases the evolution\n",
            "of RAG technologies and their application on many different\n",
            "tasks. The analysis outlines three developmental paradigms\n",
            "within the RAG framework: Naive, Advanced, and Modu-' metadata={'page': 15, 'source': '/content/RAG_survey.pdf'}\n",
            "\n",
            "page_content='a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG\n",
            "The Naive RAG research paradigm represents the earli-\n",
            "est methodology, which gained prominence shortly after the' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}\n"
          ]
        }
      ],
      "source": [
        "## TODO: Try some different settings for the retriever and output some examples\n",
        "## you can cahnge the question if you want\n",
        "## you can duplicate this cell to ouput different examples\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\n",
        "        \"k\": 5,\n",
        "        \"fetch_k\": 10,  # 在篩選文件前先列出10個候選文件\n",
        "        \"lambda_mult\": 0.5  # 在相似度與多元性之間取平衡\n",
        "    }\n",
        ")\n",
        "retrived_docs= retriever.invoke(\"what is this paper about?\")\n",
        "for doc in retrived_docs:\n",
        "    print()\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCuvzJpqzJu4"
      },
      "source": [
        "Discussion:\n",
        "\n",
        "此處實驗三種方式來提取文件\n",
        "\n",
        "1. 依照相似度分數來提取5筆文件:單純提取相似度分數排名前五名的文件，因為都固定會提五筆文件，當相似度分數都很低時，可能就會提取五筆都是相似度低的結果。\n",
        "2. 提高相似度分數門檻:對於相似度分數設下門檻，會過濾掉品質不好的答案，此時就不一定會再抓取五筆文件，而是只會抓取有到門檻的文件，會保證結果之間的高關聯性。\n",
        "3. MMR：兼顧多樣性與準確性的方式，一樣會產出五筆文件，但是在過濾之前會先有候選名單，此處設定為10筆，再透過lambda_mult這個參數來決定要以偏向準確、多樣性，或是兩者兼顧，此處設定為0.5就是兩者兼顧。設定為0就是只考慮多樣性、1則是只注重準確性。\n",
        "\n",
        "\n",
        "在不同的情境之下，需要的設定也不一樣，今天若單純考慮相似度，則以第一種方式來設定k值；如果資料庫中有很多不相關的文件，則就要以第二種方式，對相似度分數設下門檻來過濾掉不必要的資訊；如果要進行不同面向的解釋，又要避免重複，則可以考慮第三種MMR的提取方式。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odTdrQK3zJu4"
      },
      "source": [
        "## Part2 Multi-Query RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoJK1A7DzJu4"
      },
      "source": [
        "### (a) Prompt Template for Multi-Query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ_gTTxtzJu4"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "## TODO: Please design a prompt template that instructs the language model to respond to questions from multiple perspectives.\n",
        "template = \"\"\"\n",
        "You are tasked with improving the coverage of a search query by generating related queries.\n",
        "Given the original question, generate multiple related questions that approach the topic from different perspectives.\n",
        "These perspectives could include:\n",
        "- Broader or narrower interpretations.\n",
        "- Alternative phrasing or synonyms.\n",
        "- Questions targeting specific subtopics or contexts.\n",
        "- Questions considering different viewpoints or assumptions.\n",
        "\n",
        "Original question: {question}\n",
        "\n",
        "Please provide 5 diverse related queries:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5.\n",
        "\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ApA-8z_zJu4"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUCwgX_AzJu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3e704a-034c-4144-b412-473d90029d39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here are 5 diverse related questions that approach the topic from different perspectives:',\n",
              " '',\n",
              " '1. **Broader interpretation:** What topics or subjects might be covered in a paper of this type?',\n",
              " '',\n",
              " 'This question broadens the scope to consider what types of papers might be similar to the one being asked about, rather than focusing solely on its content.',\n",
              " '',\n",
              " '2. **Alternative phrasing:** Can someone summarize the main points of this paper in 50 words or less?',\n",
              " '',\n",
              " 'This question encourages an alternative way of approaching the topic by asking for a concise summary, which could reveal key takeaways and main ideas.',\n",
              " '',\n",
              " '3. **Subtopic-specific query:** What specific aspect of the paper does it focus on, such as methodology, results, or conclusions?',\n",
              " '',\n",
              " 'This question targets a specific subtopic within the paper, encouraging the generation of questions that explore a particular facet of the content.',\n",
              " '',\n",
              " '4. **Alternative viewpoint:** How does this paper relate to current debates or discussions in [industry/field]?',\n",
              " '',\n",
              " 'This question considers different viewpoints and perspectives by asking how the paper contributes to ongoing conversations and debates in a particular field.',\n",
              " '',\n",
              " '5. **Assumption-based query:** Does the author assume a specific level of background knowledge about [topic] when presenting their findings?',\n",
              " '',\n",
              " 'This question questions an assumption made by the author, encouraging the generation of related queries that consider the context in which the paper is presented, such as assumptions about prior knowledge or expertise.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# You may generate some queries here to see if the queries diverse enough\n",
        "question = \"What is this paper about?\"\n",
        "generate_queries.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "設計完提示(prompt)之後，此處嘗試產生一些查詢(query)來看看結果，第一個問題是關於讀者如何去詮釋內容，是偏向後設分析的方法，且假設讀者是具有學術背景的。第二個問題是在不同領域中如何找出同義詞，和第一個問題著重的點就不一樣，整體像產出的查詢內容最後一段說的，這些相關的查詢針對問題(What is this paper about?)都有不同的角度，像是讀者詮釋、語言學上的差異、數位時代下的理解、文化背景、批判思考等，所以應該能改善提取內容的廣度。\n"
      ],
      "metadata": {
        "id": "JLwAiRS-iKoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Qgeya9zJu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536e6584-4c68-4604-fd63-553dbc110ad9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is this paper about?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union # 輸入單一查詢，但會產生多個查詢\n",
        "docs = retrieval_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH_clTM2zJu5"
      },
      "source": [
        "### (b) Multi-Query RAG Chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U92oiYeezJu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff4db7c-b6fb-4e2a-993b-650ff2b9dca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the text, it does not appear that implementing RAG is particularly difficult. In fact, the paper highlights \"state-of-the-art technologies\" and a new evaluation framework for RAG systems, suggesting that research has made significant progress in understanding and improving the framework.\n",
            "\n",
            "However, there are some challenges mentioned in the paper that may make implementing RAG more complicated than simply following a recipe. For example:\n",
            "\n",
            "* Addressing the semantic gap between questions and documents\n",
            "* Handling cases where external knowledge retrieval is necessary but also when it's not\n",
            "* Adapting to different tasks and domains\n",
            "\n",
            "Overall, while implementing RAG may require some technical expertise and creativity, there doesn't seem to be an indication that it's inherently difficult.\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import Document\n",
        "# RAG(最後回答問題時需要的模板)\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "# # TODO: Coctruct a Multi-Query RAG Chain.\n",
        "# Hint1: use the retrieval_chain in this chain\n",
        "# Hint2: consider the format of the prompt above and also use it in the chain\n",
        "multi_query_rag_chain = (\n",
        "\n",
        "    {\"context\": retrieval_chain,\n",
        "    \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "# 使用兩個不一樣的問題來測試模型回答表現\n",
        "# question = \"What is this paper about?\"  #單純詢問論文內容\n",
        "question = \"Is it difficult to implement RAG\"\n",
        "answer = multi_query_rag_chain.invoke({\"question\": question})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzhW_r_vzJu5"
      },
      "source": [
        "### (c) Example Comparisons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJcyoFIqzJu5"
      },
      "outputs": [],
      "source": [
        "## TODO:  show a standard RAG output example alongside a multi-query RAG output example.\n",
        "# Hint1: You may adjust the question to highlight the advantages of multi-query RAG over standard RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Yr_De5zJu5"
      },
      "source": [
        "Discussion:\n",
        "\n",
        "1.  標準RAG(單一查詢)：在詢問這篇論文的內容時，標準的RAG產出的內容已經能符合需求，因此在後來嘗試詢問\"Is it difficult to implement RAG?\"(要實現RAG會很難嗎？)來看看模型的回應，回應如下:\n",
        "\n",
        "  The text does not explicitly state that implementing RAG is difficult. However, it mentions some challenges that RAG currently faces (Section VII), such as:\\n\\n* The need for continuous knowledge updates\\n* Integration of domain-specific information\\n* Wide adoption of ChatGPT\\n\\nIt also discusses the difficulties in addressing these challenges and points out prospective avenues for research and development.\\n\\nAdditionally, the text mentions that there are different versions of RAG paradigms (Naive RAG, Advanced RAG, Modular RAG), which might imply that implementing RAG can be complex due to the need to understand and integrate these different components.\n",
        "\n",
        "  標準RAG有去找一些關鍵字像是challenge，最後也有提到一些不同版本的RAG，但是沒有很明確回答實現RAG會不會很困難。\n",
        "\n",
        "2.  多重查詢RAG：一樣詢問實現RAG會不會很難，回答如下\n",
        "\n",
        "  Based on the text, it does not appear that implementing RAG is particularly difficult. In fact, the paper highlights \"state-of-the-art technologies\" and a new evaluation framework for RAG systems, suggesting that research has made significant progress in understanding and improving the framework.\n",
        "\n",
        "  However, there are some challenges mentioned in the paper that may make implementing RAG more complicated than simply following a recipe. For example:\n",
        "\n",
        "  * Addressing the semantic gap between questions and documents\n",
        "  * Handling cases where external knowledge retrieval is necessary but also when it's not\n",
        "  * Adapting to different tasks and domains\n",
        "\n",
        "  Overall, while implementing RAG may require some technical expertise and creativity, there doesn't seem to be an indication that it's inherently difficult.\n",
        "\n",
        "  這裡首先注意到回答內容比較精簡，不會像標準RAG還會泛論這篇論文的內容，多重查詢RAG的回答直接告訴使用者要實現RAG不會很難，並且列出可能遇到的挑戰。比起標準RAG，多重查詢使用多個角度來處理使用者詢問的問題，因此在回答上的表現比較肯定也比較全面，然後也能提供使用者較好的答案，確實有比標準RAG的表現好。  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7_GOVgczJu5"
      },
      "source": [
        "## Part3 RAG Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dod-ncsnzJu5"
      },
      "outputs": [],
      "source": [
        "# TODO: Use the same templat as Part2\n",
        "template = \"\"\"\n",
        "You are tasked with improving the coverage of a search query by generating related queries.\n",
        "Given the original question, generate multiple related questions that approach the topic from different perspectives.\n",
        "These perspectives could include:\n",
        "- Broader or narrower interpretations.\n",
        "- Alternative phrasing or synonyms.\n",
        "- Questions targeting specific subtopics or contexts.\n",
        "- Questions considering different viewpoints or assumptions.\n",
        "\n",
        "Original question: {question}\n",
        "\n",
        "Please provide 5 diverse related queries:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5.\n",
        "\"\"\"\n",
        "\n",
        "# template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "# {context}\n",
        "\n",
        "# Question: {question}\n",
        "# \"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdKhCpQKzJu5"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk7T5VMSzJu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b28bbc0-95ee-4a4f-dfe2-bb5df7b705eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here are five diverse related queries that approach the topic from different perspectives:',\n",
              " '',\n",
              " '1. **Narrower interpretation**: What are some challenges in implementing a Readability Analysis Graph (RAG) for a specific industry or domain?',\n",
              " '',\n",
              " 'This query targets a specific context, such as an industry or domain, which may require more tailored approaches to RAG implementation.',\n",
              " '',\n",
              " '2. **Alternative phrasing or synonyms**: How do I create a RAG model from scratch using Python, and what are some common pitfalls to avoid?',\n",
              " '',\n",
              " 'This query uses different wording and focuses on the technical implementation details of creating a RAG model using Python.',\n",
              " '',\n",
              " '3. **Targeting specific subtopics or contexts**: Is there an RAG-based approach that can be applied to sentiment analysis for social media posts, and if so, how does it work?',\n",
              " '',\n",
              " 'This query targets a specific application of RAG (sentiment analysis) in a particular context (social media posts), which may require additional techniques or adaptations.',\n",
              " '',\n",
              " '4. **Considering different viewpoints or assumptions**: What are the implications of using a graph-based approach to RAG implementation, and how might this impact performance in certain scenarios?',\n",
              " '',\n",
              " 'This query takes a more theoretical stance, considering the potential consequences of adopting a specific approach (graph-based) and its implications for performance.',\n",
              " '',\n",
              " '5. **Broader interpretation**: How does RAG implementation relate to broader topics like information retrieval, natural language processing, or knowledge representation?',\n",
              " '',\n",
              " 'This query takes a step back from the technical details of RAG implementation and considers its connections to other areas of research, such as information retrieval, NLP, or knowledge representation.']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "question = \"What is this paper about?\"\n",
        "generate_queries.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyjWzOdzJu5"
      },
      "source": [
        "### (a) Implement Reciprocal Rank Fusion (RRF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr6MUbrazJu5"
      },
      "outputs": [],
      "source": [
        "def reciprocal_rank_fusion(results: list[list], c=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "          ## TODO:  Implement Reciprocal Rank Fusion here\n",
        "          doc_str = dumps(doc) #要當成key來使用(不然document為unhashable)\n",
        "          score = 1 / (c+rank)\n",
        "          if doc_str not in fused_scores:\n",
        "            fused_scores[doc_str] = 0\n",
        "          fused_scores[doc_str] += score\n",
        "\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gdlr392zJu5"
      },
      "source": [
        "### (b) RRF Example and c-Value Discussion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px6aIfZhzJu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d877e20-9d9f-4cac-a4f1-412f2459eb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for c = 10:\n",
            "  0.016129032258064516: 0.3636\n",
            "  0.015625: 0.3636\n",
            "  0.015873015873015872: 0.2727\n",
            "  0.032266458495966696: 0.1818\n",
            "  0.016666666666666666: 0.1818\n",
            "  0.01639344262295082: 0.1818\n",
            "  page_content='structural and semantic nuances. The initial phase focuses on\n",
            "the retriever, where contrastive learning is harnessed to refine\n",
            "the query and document embeddings.\n",
            "Aligning LLM outputs with human or retriever preferences\n",
            "through reinforcement learning is a potential approach. For\n",
            "instance, manually annotating the final generated answers\n",
            "and then providing feedback through reinforcement learning.\n",
            "In addition to aligning with human preferences, it is also' metadata={'page': 9, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='In addition to extracting metadata from the original doc-\n",
            "uments, metadata can also be artificially constructed. For\n",
            "example, adding summaries of paragraph, as well as intro-\n",
            "ducing hypothetical questions. This method is also known as\n",
            "Reverse HyDE. Specifically, using LLM to generate questions\n",
            "that can be answered by the document, then calculating the\n",
            "similarity between the original question and the hypothetical\n",
            "question during retrieval to reduce the semantic gap between' metadata={'page': 7, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='decides when to activate retrieval, or alternatively, a predefined\n",
            "threshold may trigger the process. During retrieval, the gen-\n",
            "erator conducts a fragment-level beam search across multiple\n",
            "paragraphs to derive the most coherent sequence. Critic scores\n",
            "are used to update the subdivision scores, with the flexibility\n",
            "to adjust these weights during inference, tailoring the model’s\n",
            "behavior. Self-RAG’s design obviates the need for additional' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='is segmented into smaller, digestible chunks. Chunks are then\n",
            "encoded into vector representations using an embedding model\n",
            "and stored in vector database. This step is crucial for enabling\n",
            "efficient similarity searches in the subsequent retrieval phase.\n",
            "Retrieval. Upon receipt of a user query, the RAG system\n",
            "employs the same encoding model utilized during the indexing\n",
            "phase to transform the query into a vector representation.\n",
            "It then computes the similarity scores between the query' metadata={'page': 2, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='retrieving pertinent knowledge, which in turn facilitates the\n",
            "generation of improved responses in subsequent iterations.\n",
            "B. Recursive Retrieval\n",
            "Recursive retrieval is often used in information retrieval and\n",
            "NLP to improve the depth and relevance of search results.\n",
            "The process involves iteratively refining search queries based\n",
            "on the results obtained from previous searches. Recursive\n",
            "Retrieval aims to enhance the search experience by gradu-' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='or reinforcement learning) [26]. For example, this can involve\n",
            "fine-tuning the retriever for better retrieval results, fine-tuning\n",
            "the generator for more personalized outputs, or engaging in\n",
            "collaborative fine-tuning [27].\n",
            "D. RAG vs Fine-tuning\n",
            "The augmentation of LLMs has attracted considerable atten-\n",
            "tion due to their growing prevalence. Among the optimization\n",
            "methods for LLMs, RAG is often compared with Fine-tuning\n",
            "(FT) and prompt engineering. Each method has distinct charac-' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='VIII. C ONCLUSION\n",
            "The summary of this paper, as depicted in Figure 6, empha-\n",
            "sizes RAG’s significant advancement in enhancing the capa-\n",
            "bilities of LLMs by integrating parameterized knowledge from\n",
            "language models with extensive non-parameterized data from\n",
            "external knowledge bases. The survey showcases the evolution\n",
            "of RAG technologies and their application on many different\n",
            "tasks. The analysis outlines three developmental paradigms\n",
            "within the RAG framework: Naive, Advanced, and Modu-' metadata={'page': 15, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='analyzes the three augmentation processes. Section VI focuses\n",
            "on RAG’s downstream tasks and evaluation system. Sec-\n",
            "tion VII mainly discusses the challenges that RAG currently\n",
            "faces and its future development directions. At last, the paper\n",
            "concludes in Section VIII.\n",
            "II. O VERVIEW OF RAG\n",
            "A typical application of RAG is illustrated in Figure 2.\n",
            "Here, a user poses a question to ChatGPT about a recent,\n",
            "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='frameworks, which includes the retrieval, the generation and the\n",
            "augmentation techniques. The paper highlights the state-of-the-\n",
            "art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG\n",
            "systems. Furthermore, this paper introduces up-to-date evalua-\n",
            "tion framework and benchmark. At the end, this article delineates\n",
            "the challenges currently faced and points out prospective avenues\n",
            "for research and development 1.' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='including traditional single-hop/multi-hop QA, multiple-\n",
            "choice, domain-specific QA as well as long-form scenarios\n",
            "suitable for RAG. In addition to QA, RAG is continuously\n",
            "being expanded into multiple downstream tasks, such as Infor-\n",
            "mation Extraction (IE), dialogue generation, code search, etc.\n",
            "The main downstream tasks of RAG and their corresponding\n",
            "datasets are summarized in Table II.\n",
            "B. Evaluation Target\n",
            "Historically, RAG models assessments have centered on' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='4\n",
            "Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\n",
            "Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\n",
            "chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='for continuous knowledge updates and integration of domain-\n",
            "specific information. RAG synergistically merges LLMs’ intrin-\n",
            "sic knowledge with the vast, dynamic repositories of external\n",
            "databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing\n",
            "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
            "It meticulously scrutinizes the tripartite foundation of RAG\n",
            "frameworks, which includes the retrieval, the generation and the' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='overly rely on augmented information, leading to outputs that\n",
            "simply echo retrieved content without adding insightful or\n",
            "synthesized information.\n",
            "B. Advanced RAG\n",
            "Advanced RAG introduces specific improvements to over-\n",
            "come the limitations of Naive RAG. Focusing on enhancing re-\n",
            "trieval quality, it employs pre-retrieval and post-retrieval strate-\n",
            "gies. To tackle the indexing issues, Advanced RAG refines\n",
            "its indexing techniques through the use of a sliding window' metadata={'page': 2, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='with a tailored textbook for information retrieval, ideal for pre-\n",
            "cise information retrieval tasks. In contrast, FT is comparable\n",
            "to a student internalizing knowledge over time, suitable for\n",
            "scenarios requiring replication of specific structures, styles, or\n",
            "formats.\n",
            "RAG excels in dynamic environments by offering real-\n",
            "time knowledge updates and effective utilization of external\n",
            "knowledge sources with high interpretability. However, it' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='previously reliant on RAG, can now incorporate the entire\n",
            "document directly into the prompt. This has also sparked\n",
            "discussions on whether RAG is still necessary when LLMs\n",
            "8https://www.trulens.org/trulens eval/core concepts rag triad/\n",
            "9https://kimi.moonshot.cn\n",
            "are not constrained by context. In fact, RAG still plays an\n",
            "irreplaceable role. On one hand, providing LLMs with a\n",
            "large amount of context at once will significantly impact its\n",
            "inference speed, while chunked retrieval and on-demand input' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='integration of RAG within LLMs. This paper considers both\n",
            "technical paradigms and research methods, summarizing three\n",
            "main research paradigms from over 100 RAG studies, and\n",
            "analyzing key technologies in the core stages of “Retrieval,”\n",
            "“Generation,” and “Augmentation.” On the other hand, current\n",
            "research tends to focus more on methods, lacking analysis and\n",
            "summarization of how to evaluate RAG. This paper compre-\n",
            "hensively reviews the downstream tasks, datasets, benchmarks,' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\n",
            "generation; it includes methods such as iterative and adaptive retrieval.\n",
            "Pre-retrieval process. In this stage, the primary focus is\n",
            "on optimizing the indexing structure and the original query.\n",
            "The goal of optimizing indexing is to enhance the quality of\n",
            "the content being indexed. This involves strategies: enhancing' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='and Sentence pairs [38]. Detailed information is illustrated in\n",
            "Table I.\n",
            "B. Indexing Optimization\n",
            "In the Indexing phase, documents will be processed, seg-\n",
            "mented, and transformed into Embeddings to be stored in a\n",
            "vector database. The quality of index construction determines\n",
            "whether the correct context can be obtained in the retrieval\n",
            "phase.\n",
            "1) Chunking Strategy: The most common method is to split\n",
            "the document into chunks on a fixed number of tokens (e.g.,' metadata={'page': 7, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='the content being indexed. This involves strategies: enhancing\n",
            "data granularity, optimizing index structures, adding metadata,\n",
            "alignment optimization, and mixed retrieval. While the goal\n",
            "of query optimization is to make the user’s original question\n",
            "clearer and more suitable for the retrieval task. Common\n",
            "methods include query rewriting query transformation, query\n",
            "expansion and other techniques [7], [9]–[11].\n",
            "Post-Retrieval Process. Once relevant context is retrieved,' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='number included in the context. The research findings reveal\n",
            "that including irrelevant documents can unexpectedly increase\n",
            "accuracy by over 30%, contradicting the initial assumption\n",
            "of reduced quality. These results underscore the importance\n",
            "of developing specialized strategies to integrate retrieval with\n",
            "language generation models, highlighting the need for further\n",
            "research and exploration into the robustness of RAG.\n",
            "C. Hybrid Approaches\n",
            "Combining RAG with fine-tuning is emerging as a leading' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='9\n",
            "2) Query Transformation: The core concept is to retrieve\n",
            "chunks based on a transformed query instead of the user’s\n",
            "original query.\n",
            "Query Rewrite.The original queries are not always optimal\n",
            "for LLM retrieval, especially in real-world scenarios. There-\n",
            "fore, we can prompt LLM to rewrite the queries. In addition to\n",
            "using LLM for query rewriting, specialized smaller language\n",
            "models, such as RRR (Rewrite-retrieve-read) [7]. The imple-\n",
            "mentation of the query rewrite method in the Taobao, known' metadata={'page': 8, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='centers on the generator’s capacity to synthesize coherent and\n",
            "relevant answers from the retrieved context. This evaluation\n",
            "can be categorized based on the content’s objectives: unlabeled\n",
            "and labeled content. For unlabeled content, the evaluation\n",
            "encompasses the faithfulness, relevance, and non-harmfulness\n",
            "of the generated answers. In contrast, for labeled content,\n",
            "the focus is on the accuracy of the information produced by\n",
            "the model [161]. Additionally, both retrieval and generation' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='inference speed, while chunked retrieval and on-demand input\n",
            "can significantly improve operational efficiency. On the other\n",
            "hand, RAG-based generation can quickly locate the original\n",
            "references for LLMs to help users verify the generated an-\n",
            "swers. The entire retrieval and reasoning process is observable,\n",
            "while generation solely relying on long context remains a\n",
            "black box. Conversely, the expansion of context provides new\n",
            "opportunities for the development of RAG, enabling it to' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='question, form a comprehensive prompt that empowers LLMs\n",
            "to generate a well-informed answer.\n",
            "The RAG research paradigm is continuously evolving, and\n",
            "we categorize it into three stages: Naive RAG, Advanced\n",
            "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
            "RAG method are cost-effective and surpass the performance\n",
            "of the native LLM, they also exhibit several limitations.\n",
            "The development of Advanced RAG and Modular RAG is\n",
            "a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='models outperform larger ones, is particularly intriguing and\n",
            "merits further investigation.\n",
            "E. Production-Ready RAG\n",
            "RAG’s practicality and alignment with engineering require-\n",
            "ments have facilitated its adoption. However, enhancing re-\n",
            "trieval efficiency, improving document recall in large knowl-\n",
            "edge bases, and ensuring data security—such as preventing\n",
            "10https://github.com/inverse-scaling/prize\n",
            "inadvertent disclosure of document sources or metadata by' metadata={'page': 14, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG\n",
            "The Naive RAG research paradigm represents the earli-\n",
            "est methodology, which gained prominence shortly after the' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='adjusting interaction flow among existing ones, enhancing its\n",
            "applicability across different tasks.\n",
            "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
            "leverage the LLM’s capabilities to refine retrieval queries\n",
            "through a rewriting module and a LM-feedback mechanism\n",
            "to update rewriting model., improving task performance.\n",
            "Similarly, approaches like Generate-Read [13] replace tradi-\n",
            "tional retrieval with LLM-generated content, while Recite-' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='and queries with enhanced precision and flexibility.\n",
            "2) New Patterns: Modular RAG offers remarkable adapt-\n",
            "ability by allowing module substitution or reconfiguration\n",
            "to address specific challenges. This goes beyond the fixed\n",
            "structures of Naive and Advanced RAG, characterized by a\n",
            "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
            "RAG expands this flexibility by integrating new modules or\n",
            "adjusting interaction flow among existing ones, enhancing its\n",
            "applicability across different tasks.' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='iterations. However, it may be affected by semantic discon-\n",
            "tinuity and the accumulation of irrelevant information. ITER-\n",
            "RETGEN [14] employs a synergistic approach that lever-\n",
            "ages “retrieval-enhanced generation” alongside “generation-\n",
            "enhanced retrieval” for tasks that necessitate the reproduction\n",
            "of specific information. The model harnesses the content\n",
            "required to address the input task as a contextual basis for\n",
            "retrieving pertinent knowledge, which in turn facilitates the' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  page_content='the user’s needs are not entirely clear from the outset or where\n",
            "the information sought is highly specialized or nuanced. The\n",
            "recursive nature of the process allows for continuous learning\n",
            "and adaptation to the user’s requirements, often resulting in\n",
            "improved satisfaction with the search outcomes.\n",
            "To address specific data scenarios, recursive retrieval and\n",
            "multi-hop retrieval techniques are utilized together. Recursive\n",
            "retrieval involves a structured index to process and retrieve' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.1000\n",
            "  0.19383000512032766: 0.0909\n",
            "  0.18878968253968254: 0.0909\n",
            "  0.17512360135310953: 0.0909\n",
            "  0.16666666666666666: 0.0909\n",
            "  0.16393442622950818: 0.0909\n",
            "  0.09787425083729949: 0.0909\n",
            "  0.09732861591666457: 0.0909\n",
            "  0.09529569892473118: 0.0909\n",
            "  0.04893312516263336: 0.0909\n",
            "  0.048924731182795694: 0.0909\n",
            "  0.04762704813108039: 0.0909\n",
            "  0.03252247488101534: 0.0909\n",
            "  0.03229166666666666: 0.0909\n",
            "\n",
            "Results for c = 60:\n",
            "  0.016129032258064516: 0.0656\n",
            "  0.015625: 0.0656\n",
            "  0.015873015873015872: 0.0492\n",
            "  0.032266458495966696: 0.0328\n",
            "  0.016666666666666666: 0.0328\n",
            "  0.01639344262295082: 0.0328\n",
            "  page_content='structural and semantic nuances. The initial phase focuses on\n",
            "the retriever, where contrastive learning is harnessed to refine\n",
            "the query and document embeddings.\n",
            "Aligning LLM outputs with human or retriever preferences\n",
            "through reinforcement learning is a potential approach. For\n",
            "instance, manually annotating the final generated answers\n",
            "and then providing feedback through reinforcement learning.\n",
            "In addition to aligning with human preferences, it is also' metadata={'page': 9, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='In addition to extracting metadata from the original doc-\n",
            "uments, metadata can also be artificially constructed. For\n",
            "example, adding summaries of paragraph, as well as intro-\n",
            "ducing hypothetical questions. This method is also known as\n",
            "Reverse HyDE. Specifically, using LLM to generate questions\n",
            "that can be answered by the document, then calculating the\n",
            "similarity between the original question and the hypothetical\n",
            "question during retrieval to reduce the semantic gap between' metadata={'page': 7, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='decides when to activate retrieval, or alternatively, a predefined\n",
            "threshold may trigger the process. During retrieval, the gen-\n",
            "erator conducts a fragment-level beam search across multiple\n",
            "paragraphs to derive the most coherent sequence. Critic scores\n",
            "are used to update the subdivision scores, with the flexibility\n",
            "to adjust these weights during inference, tailoring the model’s\n",
            "behavior. Self-RAG’s design obviates the need for additional' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='is segmented into smaller, digestible chunks. Chunks are then\n",
            "encoded into vector representations using an embedding model\n",
            "and stored in vector database. This step is crucial for enabling\n",
            "efficient similarity searches in the subsequent retrieval phase.\n",
            "Retrieval. Upon receipt of a user query, the RAG system\n",
            "employs the same encoding model utilized during the indexing\n",
            "phase to transform the query into a vector representation.\n",
            "It then computes the similarity scores between the query' metadata={'page': 2, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='retrieving pertinent knowledge, which in turn facilitates the\n",
            "generation of improved responses in subsequent iterations.\n",
            "B. Recursive Retrieval\n",
            "Recursive retrieval is often used in information retrieval and\n",
            "NLP to improve the depth and relevance of search results.\n",
            "The process involves iteratively refining search queries based\n",
            "on the results obtained from previous searches. Recursive\n",
            "Retrieval aims to enhance the search experience by gradu-' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='or reinforcement learning) [26]. For example, this can involve\n",
            "fine-tuning the retriever for better retrieval results, fine-tuning\n",
            "the generator for more personalized outputs, or engaging in\n",
            "collaborative fine-tuning [27].\n",
            "D. RAG vs Fine-tuning\n",
            "The augmentation of LLMs has attracted considerable atten-\n",
            "tion due to their growing prevalence. Among the optimization\n",
            "methods for LLMs, RAG is often compared with Fine-tuning\n",
            "(FT) and prompt engineering. Each method has distinct charac-' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='VIII. C ONCLUSION\n",
            "The summary of this paper, as depicted in Figure 6, empha-\n",
            "sizes RAG’s significant advancement in enhancing the capa-\n",
            "bilities of LLMs by integrating parameterized knowledge from\n",
            "language models with extensive non-parameterized data from\n",
            "external knowledge bases. The survey showcases the evolution\n",
            "of RAG technologies and their application on many different\n",
            "tasks. The analysis outlines three developmental paradigms\n",
            "within the RAG framework: Naive, Advanced, and Modu-' metadata={'page': 15, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='analyzes the three augmentation processes. Section VI focuses\n",
            "on RAG’s downstream tasks and evaluation system. Sec-\n",
            "tion VII mainly discusses the challenges that RAG currently\n",
            "faces and its future development directions. At last, the paper\n",
            "concludes in Section VIII.\n",
            "II. O VERVIEW OF RAG\n",
            "A typical application of RAG is illustrated in Figure 2.\n",
            "Here, a user poses a question to ChatGPT about a recent,\n",
            "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='frameworks, which includes the retrieval, the generation and the\n",
            "augmentation techniques. The paper highlights the state-of-the-\n",
            "art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG\n",
            "systems. Furthermore, this paper introduces up-to-date evalua-\n",
            "tion framework and benchmark. At the end, this article delineates\n",
            "the challenges currently faced and points out prospective avenues\n",
            "for research and development 1.' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='including traditional single-hop/multi-hop QA, multiple-\n",
            "choice, domain-specific QA as well as long-form scenarios\n",
            "suitable for RAG. In addition to QA, RAG is continuously\n",
            "being expanded into multiple downstream tasks, such as Infor-\n",
            "mation Extraction (IE), dialogue generation, code search, etc.\n",
            "The main downstream tasks of RAG and their corresponding\n",
            "datasets are summarized in Table II.\n",
            "B. Evaluation Target\n",
            "Historically, RAG models assessments have centered on' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='4\n",
            "Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\n",
            "Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\n",
            "chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='for continuous knowledge updates and integration of domain-\n",
            "specific information. RAG synergistically merges LLMs’ intrin-\n",
            "sic knowledge with the vast, dynamic repositories of external\n",
            "databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing\n",
            "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
            "It meticulously scrutinizes the tripartite foundation of RAG\n",
            "frameworks, which includes the retrieval, the generation and the' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='overly rely on augmented information, leading to outputs that\n",
            "simply echo retrieved content without adding insightful or\n",
            "synthesized information.\n",
            "B. Advanced RAG\n",
            "Advanced RAG introduces specific improvements to over-\n",
            "come the limitations of Naive RAG. Focusing on enhancing re-\n",
            "trieval quality, it employs pre-retrieval and post-retrieval strate-\n",
            "gies. To tackle the indexing issues, Advanced RAG refines\n",
            "its indexing techniques through the use of a sliding window' metadata={'page': 2, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='with a tailored textbook for information retrieval, ideal for pre-\n",
            "cise information retrieval tasks. In contrast, FT is comparable\n",
            "to a student internalizing knowledge over time, suitable for\n",
            "scenarios requiring replication of specific structures, styles, or\n",
            "formats.\n",
            "RAG excels in dynamic environments by offering real-\n",
            "time knowledge updates and effective utilization of external\n",
            "knowledge sources with high interpretability. However, it' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='previously reliant on RAG, can now incorporate the entire\n",
            "document directly into the prompt. This has also sparked\n",
            "discussions on whether RAG is still necessary when LLMs\n",
            "8https://www.trulens.org/trulens eval/core concepts rag triad/\n",
            "9https://kimi.moonshot.cn\n",
            "are not constrained by context. In fact, RAG still plays an\n",
            "irreplaceable role. On one hand, providing LLMs with a\n",
            "large amount of context at once will significantly impact its\n",
            "inference speed, while chunked retrieval and on-demand input' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='integration of RAG within LLMs. This paper considers both\n",
            "technical paradigms and research methods, summarizing three\n",
            "main research paradigms from over 100 RAG studies, and\n",
            "analyzing key technologies in the core stages of “Retrieval,”\n",
            "“Generation,” and “Augmentation.” On the other hand, current\n",
            "research tends to focus more on methods, lacking analysis and\n",
            "summarization of how to evaluate RAG. This paper compre-\n",
            "hensively reviews the downstream tasks, datasets, benchmarks,' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\n",
            "generation; it includes methods such as iterative and adaptive retrieval.\n",
            "Pre-retrieval process. In this stage, the primary focus is\n",
            "on optimizing the indexing structure and the original query.\n",
            "The goal of optimizing indexing is to enhance the quality of\n",
            "the content being indexed. This involves strategies: enhancing' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='and Sentence pairs [38]. Detailed information is illustrated in\n",
            "Table I.\n",
            "B. Indexing Optimization\n",
            "In the Indexing phase, documents will be processed, seg-\n",
            "mented, and transformed into Embeddings to be stored in a\n",
            "vector database. The quality of index construction determines\n",
            "whether the correct context can be obtained in the retrieval\n",
            "phase.\n",
            "1) Chunking Strategy: The most common method is to split\n",
            "the document into chunks on a fixed number of tokens (e.g.,' metadata={'page': 7, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='the content being indexed. This involves strategies: enhancing\n",
            "data granularity, optimizing index structures, adding metadata,\n",
            "alignment optimization, and mixed retrieval. While the goal\n",
            "of query optimization is to make the user’s original question\n",
            "clearer and more suitable for the retrieval task. Common\n",
            "methods include query rewriting query transformation, query\n",
            "expansion and other techniques [7], [9]–[11].\n",
            "Post-Retrieval Process. Once relevant context is retrieved,' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='number included in the context. The research findings reveal\n",
            "that including irrelevant documents can unexpectedly increase\n",
            "accuracy by over 30%, contradicting the initial assumption\n",
            "of reduced quality. These results underscore the importance\n",
            "of developing specialized strategies to integrate retrieval with\n",
            "language generation models, highlighting the need for further\n",
            "research and exploration into the robustness of RAG.\n",
            "C. Hybrid Approaches\n",
            "Combining RAG with fine-tuning is emerging as a leading' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='9\n",
            "2) Query Transformation: The core concept is to retrieve\n",
            "chunks based on a transformed query instead of the user’s\n",
            "original query.\n",
            "Query Rewrite.The original queries are not always optimal\n",
            "for LLM retrieval, especially in real-world scenarios. There-\n",
            "fore, we can prompt LLM to rewrite the queries. In addition to\n",
            "using LLM for query rewriting, specialized smaller language\n",
            "models, such as RRR (Rewrite-retrieve-read) [7]. The imple-\n",
            "mentation of the query rewrite method in the Taobao, known' metadata={'page': 8, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='centers on the generator’s capacity to synthesize coherent and\n",
            "relevant answers from the retrieved context. This evaluation\n",
            "can be categorized based on the content’s objectives: unlabeled\n",
            "and labeled content. For unlabeled content, the evaluation\n",
            "encompasses the faithfulness, relevance, and non-harmfulness\n",
            "of the generated answers. In contrast, for labeled content,\n",
            "the focus is on the accuracy of the information produced by\n",
            "the model [161]. Additionally, both retrieval and generation' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='inference speed, while chunked retrieval and on-demand input\n",
            "can significantly improve operational efficiency. On the other\n",
            "hand, RAG-based generation can quickly locate the original\n",
            "references for LLMs to help users verify the generated an-\n",
            "swers. The entire retrieval and reasoning process is observable,\n",
            "while generation solely relying on long context remains a\n",
            "black box. Conversely, the expansion of context provides new\n",
            "opportunities for the development of RAG, enabling it to' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='question, form a comprehensive prompt that empowers LLMs\n",
            "to generate a well-informed answer.\n",
            "The RAG research paradigm is continuously evolving, and\n",
            "we categorize it into three stages: Naive RAG, Advanced\n",
            "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
            "RAG method are cost-effective and surpass the performance\n",
            "of the native LLM, they also exhibit several limitations.\n",
            "The development of Advanced RAG and Modular RAG is\n",
            "a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='models outperform larger ones, is particularly intriguing and\n",
            "merits further investigation.\n",
            "E. Production-Ready RAG\n",
            "RAG’s practicality and alignment with engineering require-\n",
            "ments have facilitated its adoption. However, enhancing re-\n",
            "trieval efficiency, improving document recall in large knowl-\n",
            "edge bases, and ensuring data security—such as preventing\n",
            "10https://github.com/inverse-scaling/prize\n",
            "inadvertent disclosure of document sources or metadata by' metadata={'page': 14, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG\n",
            "The Naive RAG research paradigm represents the earli-\n",
            "est methodology, which gained prominence shortly after the' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='adjusting interaction flow among existing ones, enhancing its\n",
            "applicability across different tasks.\n",
            "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
            "leverage the LLM’s capabilities to refine retrieval queries\n",
            "through a rewriting module and a LM-feedback mechanism\n",
            "to update rewriting model., improving task performance.\n",
            "Similarly, approaches like Generate-Read [13] replace tradi-\n",
            "tional retrieval with LLM-generated content, while Recite-' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='and queries with enhanced precision and flexibility.\n",
            "2) New Patterns: Modular RAG offers remarkable adapt-\n",
            "ability by allowing module substitution or reconfiguration\n",
            "to address specific challenges. This goes beyond the fixed\n",
            "structures of Naive and Advanced RAG, characterized by a\n",
            "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
            "RAG expands this flexibility by integrating new modules or\n",
            "adjusting interaction flow among existing ones, enhancing its\n",
            "applicability across different tasks.' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='iterations. However, it may be affected by semantic discon-\n",
            "tinuity and the accumulation of irrelevant information. ITER-\n",
            "RETGEN [14] employs a synergistic approach that lever-\n",
            "ages “retrieval-enhanced generation” alongside “generation-\n",
            "enhanced retrieval” for tasks that necessitate the reproduction\n",
            "of specific information. The model harnesses the content\n",
            "required to address the input task as a contextual basis for\n",
            "retrieving pertinent knowledge, which in turn facilitates the' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  page_content='the user’s needs are not entirely clear from the outset or where\n",
            "the information sought is highly specialized or nuanced. The\n",
            "recursive nature of the process allows for continuous learning\n",
            "and adaptation to the user’s requirements, often resulting in\n",
            "improved satisfaction with the search outcomes.\n",
            "To address specific data scenarios, recursive retrieval and\n",
            "multi-hop retrieval techniques are utilized together. Recursive\n",
            "retrieval involves a structured index to process and retrieve' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.0167\n",
            "  0.19383000512032766: 0.0164\n",
            "  0.18878968253968254: 0.0164\n",
            "  0.17512360135310953: 0.0164\n",
            "  0.16666666666666666: 0.0164\n",
            "  0.16393442622950818: 0.0164\n",
            "  0.09787425083729949: 0.0164\n",
            "  0.09732861591666457: 0.0164\n",
            "  0.09529569892473118: 0.0164\n",
            "  0.04893312516263336: 0.0164\n",
            "  0.048924731182795694: 0.0164\n",
            "  0.04762704813108039: 0.0164\n",
            "  0.03252247488101534: 0.0164\n",
            "  0.03229166666666666: 0.0164\n",
            "\n",
            "Results for c = 100:\n",
            "  0.016129032258064516: 0.0396\n",
            "  0.015625: 0.0396\n",
            "  0.015873015873015872: 0.0297\n",
            "  0.032266458495966696: 0.0198\n",
            "  0.016666666666666666: 0.0198\n",
            "  0.01639344262295082: 0.0198\n",
            "  page_content='structural and semantic nuances. The initial phase focuses on\n",
            "the retriever, where contrastive learning is harnessed to refine\n",
            "the query and document embeddings.\n",
            "Aligning LLM outputs with human or retriever preferences\n",
            "through reinforcement learning is a potential approach. For\n",
            "instance, manually annotating the final generated answers\n",
            "and then providing feedback through reinforcement learning.\n",
            "In addition to aligning with human preferences, it is also' metadata={'page': 9, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='In addition to extracting metadata from the original doc-\n",
            "uments, metadata can also be artificially constructed. For\n",
            "example, adding summaries of paragraph, as well as intro-\n",
            "ducing hypothetical questions. This method is also known as\n",
            "Reverse HyDE. Specifically, using LLM to generate questions\n",
            "that can be answered by the document, then calculating the\n",
            "similarity between the original question and the hypothetical\n",
            "question during retrieval to reduce the semantic gap between' metadata={'page': 7, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='decides when to activate retrieval, or alternatively, a predefined\n",
            "threshold may trigger the process. During retrieval, the gen-\n",
            "erator conducts a fragment-level beam search across multiple\n",
            "paragraphs to derive the most coherent sequence. Critic scores\n",
            "are used to update the subdivision scores, with the flexibility\n",
            "to adjust these weights during inference, tailoring the model’s\n",
            "behavior. Self-RAG’s design obviates the need for additional' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='is segmented into smaller, digestible chunks. Chunks are then\n",
            "encoded into vector representations using an embedding model\n",
            "and stored in vector database. This step is crucial for enabling\n",
            "efficient similarity searches in the subsequent retrieval phase.\n",
            "Retrieval. Upon receipt of a user query, the RAG system\n",
            "employs the same encoding model utilized during the indexing\n",
            "phase to transform the query into a vector representation.\n",
            "It then computes the similarity scores between the query' metadata={'page': 2, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='retrieving pertinent knowledge, which in turn facilitates the\n",
            "generation of improved responses in subsequent iterations.\n",
            "B. Recursive Retrieval\n",
            "Recursive retrieval is often used in information retrieval and\n",
            "NLP to improve the depth and relevance of search results.\n",
            "The process involves iteratively refining search queries based\n",
            "on the results obtained from previous searches. Recursive\n",
            "Retrieval aims to enhance the search experience by gradu-' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='or reinforcement learning) [26]. For example, this can involve\n",
            "fine-tuning the retriever for better retrieval results, fine-tuning\n",
            "the generator for more personalized outputs, or engaging in\n",
            "collaborative fine-tuning [27].\n",
            "D. RAG vs Fine-tuning\n",
            "The augmentation of LLMs has attracted considerable atten-\n",
            "tion due to their growing prevalence. Among the optimization\n",
            "methods for LLMs, RAG is often compared with Fine-tuning\n",
            "(FT) and prompt engineering. Each method has distinct charac-' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='VIII. C ONCLUSION\n",
            "The summary of this paper, as depicted in Figure 6, empha-\n",
            "sizes RAG’s significant advancement in enhancing the capa-\n",
            "bilities of LLMs by integrating parameterized knowledge from\n",
            "language models with extensive non-parameterized data from\n",
            "external knowledge bases. The survey showcases the evolution\n",
            "of RAG technologies and their application on many different\n",
            "tasks. The analysis outlines three developmental paradigms\n",
            "within the RAG framework: Naive, Advanced, and Modu-' metadata={'page': 15, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='analyzes the three augmentation processes. Section VI focuses\n",
            "on RAG’s downstream tasks and evaluation system. Sec-\n",
            "tion VII mainly discusses the challenges that RAG currently\n",
            "faces and its future development directions. At last, the paper\n",
            "concludes in Section VIII.\n",
            "II. O VERVIEW OF RAG\n",
            "A typical application of RAG is illustrated in Figure 2.\n",
            "Here, a user poses a question to ChatGPT about a recent,\n",
            "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='frameworks, which includes the retrieval, the generation and the\n",
            "augmentation techniques. The paper highlights the state-of-the-\n",
            "art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG\n",
            "systems. Furthermore, this paper introduces up-to-date evalua-\n",
            "tion framework and benchmark. At the end, this article delineates\n",
            "the challenges currently faced and points out prospective avenues\n",
            "for research and development 1.' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='including traditional single-hop/multi-hop QA, multiple-\n",
            "choice, domain-specific QA as well as long-form scenarios\n",
            "suitable for RAG. In addition to QA, RAG is continuously\n",
            "being expanded into multiple downstream tasks, such as Infor-\n",
            "mation Extraction (IE), dialogue generation, code search, etc.\n",
            "The main downstream tasks of RAG and their corresponding\n",
            "datasets are summarized in Table II.\n",
            "B. Evaluation Target\n",
            "Historically, RAG models assessments have centered on' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='4\n",
            "Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\n",
            "Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\n",
            "chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='for continuous knowledge updates and integration of domain-\n",
            "specific information. RAG synergistically merges LLMs’ intrin-\n",
            "sic knowledge with the vast, dynamic repositories of external\n",
            "databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing\n",
            "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
            "It meticulously scrutinizes the tripartite foundation of RAG\n",
            "frameworks, which includes the retrieval, the generation and the' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='overly rely on augmented information, leading to outputs that\n",
            "simply echo retrieved content without adding insightful or\n",
            "synthesized information.\n",
            "B. Advanced RAG\n",
            "Advanced RAG introduces specific improvements to over-\n",
            "come the limitations of Naive RAG. Focusing on enhancing re-\n",
            "trieval quality, it employs pre-retrieval and post-retrieval strate-\n",
            "gies. To tackle the indexing issues, Advanced RAG refines\n",
            "its indexing techniques through the use of a sliding window' metadata={'page': 2, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='with a tailored textbook for information retrieval, ideal for pre-\n",
            "cise information retrieval tasks. In contrast, FT is comparable\n",
            "to a student internalizing knowledge over time, suitable for\n",
            "scenarios requiring replication of specific structures, styles, or\n",
            "formats.\n",
            "RAG excels in dynamic environments by offering real-\n",
            "time knowledge updates and effective utilization of external\n",
            "knowledge sources with high interpretability. However, it' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='previously reliant on RAG, can now incorporate the entire\n",
            "document directly into the prompt. This has also sparked\n",
            "discussions on whether RAG is still necessary when LLMs\n",
            "8https://www.trulens.org/trulens eval/core concepts rag triad/\n",
            "9https://kimi.moonshot.cn\n",
            "are not constrained by context. In fact, RAG still plays an\n",
            "irreplaceable role. On one hand, providing LLMs with a\n",
            "large amount of context at once will significantly impact its\n",
            "inference speed, while chunked retrieval and on-demand input' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='integration of RAG within LLMs. This paper considers both\n",
            "technical paradigms and research methods, summarizing three\n",
            "main research paradigms from over 100 RAG studies, and\n",
            "analyzing key technologies in the core stages of “Retrieval,”\n",
            "“Generation,” and “Augmentation.” On the other hand, current\n",
            "research tends to focus more on methods, lacking analysis and\n",
            "summarization of how to evaluate RAG. This paper compre-\n",
            "hensively reviews the downstream tasks, datasets, benchmarks,' metadata={'page': 0, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\n",
            "generation; it includes methods such as iterative and adaptive retrieval.\n",
            "Pre-retrieval process. In this stage, the primary focus is\n",
            "on optimizing the indexing structure and the original query.\n",
            "The goal of optimizing indexing is to enhance the quality of\n",
            "the content being indexed. This involves strategies: enhancing' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='and Sentence pairs [38]. Detailed information is illustrated in\n",
            "Table I.\n",
            "B. Indexing Optimization\n",
            "In the Indexing phase, documents will be processed, seg-\n",
            "mented, and transformed into Embeddings to be stored in a\n",
            "vector database. The quality of index construction determines\n",
            "whether the correct context can be obtained in the retrieval\n",
            "phase.\n",
            "1) Chunking Strategy: The most common method is to split\n",
            "the document into chunks on a fixed number of tokens (e.g.,' metadata={'page': 7, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='the content being indexed. This involves strategies: enhancing\n",
            "data granularity, optimizing index structures, adding metadata,\n",
            "alignment optimization, and mixed retrieval. While the goal\n",
            "of query optimization is to make the user’s original question\n",
            "clearer and more suitable for the retrieval task. Common\n",
            "methods include query rewriting query transformation, query\n",
            "expansion and other techniques [7], [9]–[11].\n",
            "Post-Retrieval Process. Once relevant context is retrieved,' metadata={'page': 3, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='number included in the context. The research findings reveal\n",
            "that including irrelevant documents can unexpectedly increase\n",
            "accuracy by over 30%, contradicting the initial assumption\n",
            "of reduced quality. These results underscore the importance\n",
            "of developing specialized strategies to integrate retrieval with\n",
            "language generation models, highlighting the need for further\n",
            "research and exploration into the robustness of RAG.\n",
            "C. Hybrid Approaches\n",
            "Combining RAG with fine-tuning is emerging as a leading' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='9\n",
            "2) Query Transformation: The core concept is to retrieve\n",
            "chunks based on a transformed query instead of the user’s\n",
            "original query.\n",
            "Query Rewrite.The original queries are not always optimal\n",
            "for LLM retrieval, especially in real-world scenarios. There-\n",
            "fore, we can prompt LLM to rewrite the queries. In addition to\n",
            "using LLM for query rewriting, specialized smaller language\n",
            "models, such as RRR (Rewrite-retrieve-read) [7]. The imple-\n",
            "mentation of the query rewrite method in the Taobao, known' metadata={'page': 8, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='centers on the generator’s capacity to synthesize coherent and\n",
            "relevant answers from the retrieved context. This evaluation\n",
            "can be categorized based on the content’s objectives: unlabeled\n",
            "and labeled content. For unlabeled content, the evaluation\n",
            "encompasses the faithfulness, relevance, and non-harmfulness\n",
            "of the generated answers. In contrast, for labeled content,\n",
            "the focus is on the accuracy of the information produced by\n",
            "the model [161]. Additionally, both retrieval and generation' metadata={'page': 11, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='inference speed, while chunked retrieval and on-demand input\n",
            "can significantly improve operational efficiency. On the other\n",
            "hand, RAG-based generation can quickly locate the original\n",
            "references for LLMs to help users verify the generated an-\n",
            "swers. The entire retrieval and reasoning process is observable,\n",
            "while generation solely relying on long context remains a\n",
            "black box. Conversely, the expansion of context provides new\n",
            "opportunities for the development of RAG, enabling it to' metadata={'page': 13, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='question, form a comprehensive prompt that empowers LLMs\n",
            "to generate a well-informed answer.\n",
            "The RAG research paradigm is continuously evolving, and\n",
            "we categorize it into three stages: Naive RAG, Advanced\n",
            "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
            "RAG method are cost-effective and surpass the performance\n",
            "of the native LLM, they also exhibit several limitations.\n",
            "The development of Advanced RAG and Modular RAG is\n",
            "a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='models outperform larger ones, is particularly intriguing and\n",
            "merits further investigation.\n",
            "E. Production-Ready RAG\n",
            "RAG’s practicality and alignment with engineering require-\n",
            "ments have facilitated its adoption. However, enhancing re-\n",
            "trieval efficiency, improving document recall in large knowl-\n",
            "edge bases, and ensuring data security—such as preventing\n",
            "10https://github.com/inverse-scaling/prize\n",
            "inadvertent disclosure of document sources or metadata by' metadata={'page': 14, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='a response to these specific shortcomings in Naive RAG.\n",
            "A. Naive RAG\n",
            "The Naive RAG research paradigm represents the earli-\n",
            "est methodology, which gained prominence shortly after the' metadata={'page': 1, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='adjusting interaction flow among existing ones, enhancing its\n",
            "applicability across different tasks.\n",
            "Innovations such as the Rewrite-Retrieve-Read [7]model\n",
            "leverage the LLM’s capabilities to refine retrieval queries\n",
            "through a rewriting module and a LM-feedback mechanism\n",
            "to update rewriting model., improving task performance.\n",
            "Similarly, approaches like Generate-Read [13] replace tradi-\n",
            "tional retrieval with LLM-generated content, while Recite-' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='and queries with enhanced precision and flexibility.\n",
            "2) New Patterns: Modular RAG offers remarkable adapt-\n",
            "ability by allowing module substitution or reconfiguration\n",
            "to address specific challenges. This goes beyond the fixed\n",
            "structures of Naive and Advanced RAG, characterized by a\n",
            "simple “Retrieve” and “Read” mechanism. Moreover, Modular\n",
            "RAG expands this flexibility by integrating new modules or\n",
            "adjusting interaction flow among existing ones, enhancing its\n",
            "applicability across different tasks.' metadata={'page': 4, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='iterations. However, it may be affected by semantic discon-\n",
            "tinuity and the accumulation of irrelevant information. ITER-\n",
            "RETGEN [14] employs a synergistic approach that lever-\n",
            "ages “retrieval-enhanced generation” alongside “generation-\n",
            "enhanced retrieval” for tasks that necessitate the reproduction\n",
            "of specific information. The model harnesses the content\n",
            "required to address the input task as a contextual basis for\n",
            "retrieving pertinent knowledge, which in turn facilitates the' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  page_content='the user’s needs are not entirely clear from the outset or where\n",
            "the information sought is highly specialized or nuanced. The\n",
            "recursive nature of the process allows for continuous learning\n",
            "and adaptation to the user’s requirements, often resulting in\n",
            "improved satisfaction with the search outcomes.\n",
            "To address specific data scenarios, recursive retrieval and\n",
            "multi-hop retrieval techniques are utilized together. Recursive\n",
            "retrieval involves a structured index to process and retrieve' metadata={'page': 10, 'source': '/content/RAG_survey.pdf'}: 0.0100\n",
            "  0.19383000512032766: 0.0099\n",
            "  0.18878968253968254: 0.0099\n",
            "  0.17512360135310953: 0.0099\n",
            "  0.16666666666666666: 0.0099\n",
            "  0.16393442622950818: 0.0099\n",
            "  0.09787425083729949: 0.0099\n",
            "  0.09732861591666457: 0.0099\n",
            "  0.09529569892473118: 0.0099\n",
            "  0.04893312516263336: 0.0099\n",
            "  0.048924731182795694: 0.0099\n",
            "  0.04762704813108039: 0.0099\n",
            "  0.03252247488101534: 0.0099\n",
            "  0.03229166666666666: 0.0099\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## TODO: Provide an example showing the documents after re-ranking using RRF.\n",
        "\n",
        "c_values = [10, 60, 100]\n",
        "results = retrieval_chain_rag_fusion.invoke(question)\n",
        "for c in c_values:\n",
        "    reranked_results = reciprocal_rank_fusion(results, c=c)\n",
        "    print(f\"Results for c = {c}:\")\n",
        "    for doc, score in reranked_results:\n",
        "        print(f\"  {doc}: {score:.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSbq9zmRzJu6"
      },
      "source": [
        "Discussion:\n",
        "\n",
        "此處一樣詢問論文內容，並使用RFF算法來將檢索內容進行排序，排名前面的檢索內容提到了關鍵字像是query、embedding、LLM等，偏低的檢索內容提到RAG的應用範圍，有點偏離問題想要得到的答案。\n",
        "\n",
        "再來討論不同的c值所帶來的影響，網路上搜尋的結果會建議使用c=60，原因是適用於大部分的資料，以及能夠處理排名低，分數差距很小的僵局，讓檢索結果可以分出高下，但這並不是絕對的，以下就使用c=10、60、100來看看結果：\n",
        "\n",
        "1. c=10：在c值偏小時，會以排名較前面的檢索內容為主，排名後面的檢索內容就不會有太多貢獻，排名前面的分數為0.36左右，後面有好幾筆沒有顯示內容的分數為0.09。\n",
        "\n",
        "2. c=60:在c為60時，分數蠻多都集中在0.01左右，排名前後的檢索內容都會有一定的貢獻，但是缺點就是差距不大。\n",
        "\n",
        "3. c=100:在c值偏大時，容易受到排名較後面的檢索結果影響，除了大家分數變得更加一致，彼此分數差距也越來越小，難以分出勝負的情形。\n",
        "\n",
        "以這次作業來說由於文件數量只有一份，所以取的值對於檢索內容不會有太大影響，但是如果文件來源多的話，c值應該就會對檢索內容有較大的影響。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3xoLkFlzJu6"
      },
      "source": [
        "### (c) RAG Fusion Chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtUBdjjVzJu6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "4dfc3bb5-00cc-4b8a-f29f-a7c298083e56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text does not explicitly state that implementing RAG is difficult, but it does mention some challenges and limitations. For example, it states that including irrelevant documents can unexpectedly increase accuracy, which may require specialized strategies for integration with language generation models. Additionally, there is a notable paucity of research dedicated to evaluating the distinct characteristics of RAG models.\\n\\nHowever, the text also mentions that combining RAG with fine-tuning is emerging as a leading approach, which suggests that implementing RAG can be done effectively with the right techniques and strategies.\\n\\nOverall, while there may be some challenges to implementing RAG, it does not seem to be an insurmountable difficulty. With careful consideration of its strengths and limitations, and the development of specialized strategies for integration with language generation models, it appears that implementing RAG is feasible.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "## TODO: Implement the RAG Fusion chain\n",
        "# Hint1: use the retrieval_chain_rag_fusion in this chain\n",
        "# Hint2: consider the format of the prompt above and also use it in the chain\n",
        "rag_fusion_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "    \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "# question = \"What is this paper about?\"\n",
        "question = \"Is it difficult to implement RAG ?\"\n",
        "\n",
        "rag_fusion_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本來利用RAG fusion chain第一個問題也是詢問這篇論文的內容，但是回答內容和標準RAG差異不大，所以還是改問\"Is it difficult to implement RAG ?\"(實現RAG會很難嗎？)回答內容比起前面多重查詢RAG更加人性化了，直接告訴我們這篇論文中沒有明確提到實現RAG難不難，而是提到挑戰與限制，可以看出RAG fusion chain在面對這種沒有明確答案的問題時，表現明顯優於標準RAG，然後也可以知道如果單純問論文內容摘要，三種方式(標準RAG、多重查詢、RAG fusion chain)的表現都差不多，但是開放式問題下還是建議使用RAG fusion chain。"
      ],
      "metadata": {
        "id": "_7EG5Cu160V9"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}